{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = pd.read_excel('cordata.xlsx') # contains all data\n",
    "df_Al = pd.read_excel('cordata_Al.xlsx') # only Al alloy class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_full.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_Al.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of entries for every alloy class in the full dataset\n",
    "for metal in df_full['Metal'].unique():\n",
    "    print(f\"{metal}: {len(df_full[df_full['Metal'] == metal])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### since Al has the biggest number it is indeed smart to start the alloy specific analysis with Al"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique compounds for full dataset\n",
    "df_full.groupby('SMILES').agg({'Efficiency':['median', 'std']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique compounds for Al only dataset\n",
    "df_Al.groupby('SMILES').agg({'Efficiency':['median', 'std']})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 402 compounds for full, 177 for only Al. How about if we work for the largest alloy group?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_counts(df, value):\n",
    "    # Count unique values in the 'value' column\n",
    "    value_counts = df[value].value_counts()\n",
    "    ranked_values = value_counts.sort_values(ascending=False)\n",
    "    print(\"Rank of unique values:\")\n",
    "    print(ranked_values)\n",
    "\n",
    "value_counts(df_full, 'Alloy')\n",
    "print()\n",
    "print(f'Unique alloys in full dataset: {len(df_full[\"Alloy\"].unique())}')\n",
    "print(f'Unique alloys in only Al dataset: {len(df_Al[\"Alloy\"].unique())}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I suggest we start from AA2024, as mild steel is too general of a category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# composition of AA2000 series is nearly identical, grab alloys from this series\n",
    "df_AA2024 = df_Al[df_Al['Alloy'].isin(['AA2024', 'AA2014', 'AA2017A'])]\n",
    "df_AA2024.groupby('SMILES').agg({'Efficiency':['median', 'std']})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 177 compounds for all Al, to 123 compounds to only AA2024."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can play around with the column names to see what unique values are in each column\n",
    "column_name = 'Time_h'\n",
    "print(df_Al[column_name].unique())\n",
    "print((len(df_Al[column_name].unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# previous counter fuinction to check distribution\n",
    "value_counts(df_Al, 'Time_h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can play around with histograms below to check distribution\n",
    "# \n",
    "# # Plot histogram\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_hist(df, value, name):\n",
    "    plt.hist(df[value], bins=50, color='skyblue', edgecolor='black')\n",
    "    plt.xlabel(f'{value}')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title(f'Histogram of {name}')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "hist_value = 'Time_h'\n",
    "plot_hist(df_full, value = hist_value, name = 'Full')\n",
    "plot_hist(df_Al, value = hist_value, name = 'Al')\n",
    "plot_hist(df_AA2024, value = hist_value, name = 'AA2024')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check filtering to see the effect of getting rid of some rows, to check whether its feasible to drop some columns\n",
    "\n",
    "def check_filter(dataset, column_name, filter):\n",
    "    print(f\"Number of entries in full dataset: {len(dataset)}\")\n",
    "    print(f\"Number of entries in full dataset with {column_name} >= {filter}: {len(dataset[dataset[column_name] >= filter])}\")\n",
    "    print(f\"Number of entries in full dataset with {column_name} < {filter}: {len(dataset[dataset[column_name] < filter])}\")\n",
    "\n",
    "\n",
    "check_filter(dataset = df_Al, column_name = 'Time_h',  filter = 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick check to check the amount of unique compounds\n",
    "filtered_df_Al = df_AA2024[df_AA2024['Time_h'] >= 1] # 123 to 121 compounds, maybe not much information loss for the small dataset...\n",
    "len(filtered_df_Al['SMILES'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now to create a new dataframe cleaned of unnecessary details for analysis. \n",
    "Entries with synergy and encapsulation is removed, then unnecessary columns dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_dataframe_AA2024(df):\n",
    "    no_synergy_df = df[df['Synergistic_inhib'] == 'No']\n",
    "    no_encapsulation_df = no_synergy_df[no_synergy_df['Encapsulated'] == 'No']\n",
    "    filtered_df = no_encapsulation_df.drop(columns=['Inhibitor', 'Number', 'Metal', 'Alloy', 'Temperature_K', 'Salt_Concentrat_M', \n",
    "                                                    'Synergistic_inhib','Synergistic_inhib_type', 'Synergistic_inhib_Concentrat_M',\n",
    "                                                    'Encapsulated', 'Methodology','Reference', 'Link', 'Contributor'])\n",
    "    return filtered_df\n",
    "\n",
    "filtered_df_AA2024 = filter_dataframe_AA2024(df_AA2024)\n",
    "len(filtered_df_AA2024['SMILES'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 611 datapoints for actual Bayesian optimization work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "filtered_df_AA2024.to_excel('filtered_AA2024.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_dataframe_Al(df):\n",
    "    no_synergy_df = df[df['Synergistic_inhib'] == 'No']\n",
    "    no_encapsulation_df = no_synergy_df[no_synergy_df['Encapsulated'] == 'No']\n",
    "    filtered_df = no_encapsulation_df.drop(columns=['Inhibitor', 'Number', 'Metal', 'Temperature_K', 'Salt_Concentrat_M', \n",
    "                                                    'Synergistic_inhib','Synergistic_inhib_type', 'Synergistic_inhib_Concentrat_M',\n",
    "                                                    'Encapsulated', 'Methodology','Reference', 'Link', 'Contributor'])\n",
    "    return filtered_df\n",
    "\n",
    "filtered_df_Al = filter_dataframe_Al(df_Al)\n",
    "len(filtered_df_Al['SMILES'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1966 datapoints, almost 3 times. Assuming we can just ignore the effect of alloy type, or find a way to featurize it, would be fun to work with. Maybe composition based?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "filtered_df_Al.to_excel('filtered_Al.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_dataframe_full(df):\n",
    "    no_synergy_df = df[df['Synergistic_Inhib_type'].isnull()]\n",
    "    filtered_df = no_synergy_df.drop(columns=['Inhibitor', 'Index', 'Mol._weight', 'Temperature_K', 'Salt_Concentrat_M',\n",
    "                                              'Synergistic_Inhib_type', 'Synergistic_Inhib_M','Methodology','Reference',\n",
    "                                              'Contributor'])\n",
    "    return filtered_df\n",
    "\n",
    "filtered_df_full = filter_dataframe_full(df_full)\n",
    "len(filtered_df_full['SMILES'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4708 datapoints, we more than double the previous. Again if we can jump to the featurization of alloy or substrate, super cool number to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "filtered_df_full.to_excel('filtered_full.xlsx', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
